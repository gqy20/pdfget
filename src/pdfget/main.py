#!/usr/bin/env python3
"""
PDFä¸‹è½½å™¨ä¸»ç¨‹åº
ç‹¬ç«‹çš„æ–‡çŒ®PDFä¸‹è½½å·¥å…·
"""

import argparse
import sys
from pathlib import Path

import logging

from .fetcher import PaperFetcher
from .config import TIMEOUT, MAX_RETRIES, DELAY, OUTPUT_DIR, LOG_LEVEL, LOG_FORMAT


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ–‡çŒ®ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¸‹è½½å•ä¸ªæ–‡çŒ®
  python main.py --doi 10.1016/j.cell.2020.01.021

  # æ‰¹é‡ä¸‹è½½ï¼ˆä»CSVæ–‡ä»¶ï¼‰
  python main.py --input dois.csv --column doi

  # æ‰¹é‡ä¸‹è½½ï¼ˆä»æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªDOIï¼‰
  python main.py --input dois.txt

  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python main.py --doi 10.1016/j.cell.2020.01.021 --output ./my_pdfs

  # å¯ç”¨è¯¦ç»†æ—¥å¿—
  python main.py --doi 10.1016/j.cell.2020.01.021 --verbose
        """
    )

    # è¾“å…¥é€‰é¡¹
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--doi", help="å•ä¸ªDOI")
    group.add_argument("--input", "-i", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆCSVæˆ–TXTï¼‰")

    # å¯é€‰å‚æ•°
    parser.add_argument("--column", "-c", default="doi",
                       help="CSVæ–‡ä»¶ä¸­çš„DOIåˆ—åï¼ˆé»˜è®¤: doiï¼‰")
    parser.add_argument("--output", "-o", default="data/pdfs",
                       help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/pdfsï¼‰")
    parser.add_argument("--cache", default="data/cache",
                       help="ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤: data/cacheï¼‰")
    parser.add_argument("--delay", type=float, default=DELAY,
                       help=f"è¯·æ±‚é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: {DELAY}ï¼‰")
    parser.add_argument("--timeout", type=int, default=TIMEOUT,
                       help=f"è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤: {TIMEOUT}ç§’ï¼‰")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else LOG_LEVEL,
        format=LOG_FORMAT
    )
    logger = logging.getLogger("PDFDownloader")

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    fetcher = PaperFetcher(cache_dir=args.cache)

    logger.info("ğŸš€ PDFä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"   è¾“å‡ºç›®å½•: {args.output}")
    logger.info(f"   ç¼“å­˜ç›®å½•: {args.cache}")
    logger.info(f"   è¯·æ±‚å»¶è¿Ÿ: {args.delay}ç§’")

    try:
        if args.doi:
            # å•ä¸ªDOIä¸‹è½½
            logger.info(f"\nğŸ“„ ä¸‹è½½å•ä¸ªæ–‡çŒ®: {args.doi}")
            result = fetcher.fetch_by_doi(args.doi, timeout=args.timeout)

            if result.get("success"):
                logger.info("âœ… ä¸‹è½½æˆåŠŸ!")
                if result.get("pdf_path"):
                    logger.info(f"   PDFè·¯å¾„: {result['pdf_path']}")
                else:
                    logger.info(f"   HTMLé“¾æ¥: {result.get('full_text_url')}")
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")

        else:
            # æ‰¹é‡ä¸‹è½½
            logger.info(f"\nğŸ“š æ‰¹é‡ä¸‹è½½: {args.input}")

            # è¯»å–DOIåˆ—è¡¨
            input_path = Path(args.input)
            if not input_path.exists():
                logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return 1

            if input_path.suffix.lower() == '.csv':
                # è¯»å–CSVæ–‡ä»¶
                import pandas as pd
                try:
                    df = pd.read_csv(input_path)
                    if args.column not in df.columns:
                        logger.error(f"âŒ CSVæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åˆ—: {args.column}")
                        return 1

                    dois = df[args.column].dropna().unique().tolist()
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªå”¯ä¸€DOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
                    return 1

            else:
                # è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªDOIï¼‰
                try:
                    with open(input_path, 'r') as f:
                        dois = [line.strip() for line in f if line.strip()]
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªDOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    return 1

            # æ‰¹é‡å¤„ç†
            results = fetcher.fetch_batch(dois, delay=args.delay)

            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if r.get("success"))
            pdf_count = sum(1 for r in results if r.get("pdf_path"))
            html_count = sum(1 for r in results if r.get("full_text_url"))

            logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            logger.info(f"   æ€»è®¡: {len(results)}")
            logger.info(f"   æˆåŠŸ: {success_count}")
            logger.info(f"   PDF: {pdf_count}")
            logger.info(f"   HTML: {html_count}")
            logger.info(f"   å¤±è´¥: {len(results) - success_count}")

            # ä¿å­˜ç»“æœ
            if success_count > 0:
                import json
                output_file = Path(args.output) / "download_results.json"
                output_file.parent.mkdir(parents=True, exist_ok=True)

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "timestamp": str(Path(__file__).stat().st_mtime),
                        "total": len(results),
                        "success": success_count,
                        "results": results
                    }, f, indent=2, ensure_ascii=False)

                logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        return 130
    except Exception as e:
        logger.error(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return 1

    logger.info("\nâœ¨ ä¸‹è½½å®Œæˆ")
    return 0


if __name__ == "__main__":
    sys.exit(main())