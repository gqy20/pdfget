#!/usr/bin/env python3
"""
PDFä¸‹è½½å™¨ä¸»ç¨‹åº
ç‹¬ç«‹çš„æ–‡çŒ®PDFä¸‹è½½å·¥å…·
"""

import argparse
import json
import logging
import time
from pathlib import Path

from .config import (
    DEFAULT_SEARCH_LIMIT,
    DEFAULT_SOURCE,
    NCBI_API_KEY,
    NCBI_EMAIL,
    TIMEOUT,
)
from .counter import PMCIDCounter
from .fetcher import PaperFetcher
from .formatter import StatsFormatter
from .logger import get_main_logger
from .manager import UnifiedDownloadManager


def log_download_stats(logger, results: list[dict]) -> dict:
    """è®°å½•ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯å¹¶è¿”å›ç»Ÿè®¡ç»“æœ"""
    success_count = sum(1 for r in results if r.get("success"))
    pdf_count = sum(1 for r in results if r.get("path"))
    html_count = sum(1 for r in results if r.get("full_text_url"))

    logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
    logger.info(f"   æ€»è®¡: {len(results)}")
    logger.info(f"   æˆåŠŸ: {success_count}")
    logger.info(f"   PDF: {pdf_count}")
    logger.info(f"   HTML: {html_count}")
    logger.info(f"   å¤±è´¥: {len(results) - success_count}")

    return {
        "total": len(results),
        "success_count": success_count,
        "pdf_count": pdf_count,
        "html_count": html_count,
    }


def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ–‡çŒ®ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ç»Ÿè®¡æ–‡çŒ®çš„PMCIDæƒ…å†µ
  python -m pdfget -s "machine learning cancer" -l 5000

  # æœç´¢å¹¶ä¸‹è½½å‰Nç¯‡æ–‡çŒ®
  python -m pdfget -s "deep learning" -l 20 -d

  # å¹¶å‘ä¸‹è½½ï¼ˆå¤šçº¿ç¨‹ï¼‰
  python -m pdfget -s "cancer immunotherapy" -l 20 -d -t 5

  # ä»CSVä¸‹è½½æ··åˆæ ‡è¯†ç¬¦ï¼ˆæ”¯æŒPMCID/PMID/DOIæ··åˆï¼Œè‡ªåŠ¨æ£€æµ‹åˆ—åï¼‰
  python -m pdfget -m identifiers.csv -t 5
  python -m pdfget -m pmcids.csv -c PMCID -l 100

  # ä¸‹è½½å•ä¸ªæ ‡è¯†ç¬¦
  python -m pdfget -m "PMC10851947"
  python -m pdfget -m "10.1016/j.cell.2020.01.021"

  # ä¸‹è½½å¤šä¸ªæ ‡è¯†ç¬¦ï¼ˆé€—å·åˆ†éš”ï¼‰
  python -m pdfget -m "PMC123456,38238491,10.1038/xxx" -t 3
        """,
    )

    # è¾“å…¥é€‰é¡¹
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-s", help="æœç´¢æ–‡çŒ®")
    group.add_argument(
        "-m",
        help="æ‰¹é‡è¾“å…¥ï¼ˆCSVæ–‡ä»¶/å•ä¸ªæ ‡è¯†ç¬¦/é€—å·åˆ†éš”åˆ—è¡¨ï¼‰ï¼Œæ”¯æŒæ··åˆPMCID/PMID/DOI",
    )

    # å¯é€‰å‚æ•°
    parser.add_argument(
        "-c",
        help="CSVåˆ—åï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹: ID>PMCID>doi>pmid>ç¬¬ä¸€åˆ—ï¼‰",
    )
    parser.add_argument("-o", default="data/pdfs", help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "-l", type=int, default=DEFAULT_SEARCH_LIMIT, help="è¦å¤„ç†çš„æ–‡çŒ®æ•°é‡"
    )
    parser.add_argument("-d", action="store_true", help="ä¸‹è½½PDF")
    parser.add_argument("-t", type=int, default=3, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼‰")
    parser.add_argument("-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument(
        "-S",
        choices=["pubmed", "europe_pmc", "both"],
        default=DEFAULT_SOURCE,
        help=f"æ•°æ®æºï¼ˆé»˜è®¤: {DEFAULT_SOURCE}ï¼‰",
    )
    parser.add_argument(
        "--format",
        choices=["console", "json", "markdown"],
        help="ç»Ÿè®¡è¾“å‡ºæ ¼å¼",
    )
    parser.add_argument("-e", help="NCBI APIé‚®ç®±ï¼ˆæé«˜è¯·æ±‚é™åˆ¶ï¼‰")
    parser.add_argument("-k", help="NCBI APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logger = get_main_logger()
    if args.v:
        logger.setLevel(logging.DEBUG)

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    fetcher = PaperFetcher(
        cache_dir="data/cache", output_dir="data/pdfs", default_source=args.S
    )

    logger.info("ğŸš€ PDFä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"   è¾“å‡ºç›®å½•: {args.o}")

    try:
        if args.s:
            # æœç´¢æ–‡çŒ®
            logger.info(f"\nğŸ” æœç´¢æ–‡çŒ®: {args.s} (æ•°æ®æº: {args.S})")

            # å¦‚æœéœ€è¦ä¸‹è½½PDFï¼Œåˆ™åªæœç´¢å°‘é‡æ–‡çŒ®
            # å¦‚æœä¸éœ€è¦ä¸‹è½½ï¼Œåˆ™è¿›è¡Œå…¨é‡ç»Ÿè®¡
            if args.d:
                # ä¸‹è½½æ¨¡å¼ï¼šåªè·å–å‰lç¯‡æ–‡çŒ®
                fetch_pmcid = args.S == "pubmed"
                papers = fetcher.search_papers(
                    args.s, limit=args.l, source=args.S, fetch_pmcid=fetch_pmcid
                )

                if not papers:
                    logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                    exit(1)

                # æ˜¾ç¤ºæœç´¢ç»“æœ
                logger.info(f"\nğŸ“Š æœç´¢ç»“æœ ({len(papers)} ç¯‡):")
                for i, paper in enumerate(papers, 1):
                    logger.info(f"\n{i}. {paper['title']}")
                    logger.info(
                        f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}"
                    )
                    logger.info(f"   æœŸåˆŠ: {paper['journal']} ({paper['year']})")
                    if paper["doi"]:
                        logger.info(f"   DOI: {paper['doi']}")
                    logger.info(f"   PMCID: {paper.get('pmcid', 'æ— ')}")
                    logger.info(f"   å¼€æ”¾è·å–: {'æ˜¯' if paper.get('pmcid') else 'å¦'}")

                # ä¿å­˜æœç´¢ç»“æœ
                search_results_file = (
                    Path(args.o) / f"search_results_{int(time.time())}.json"
                )
                search_results_file.parent.mkdir(parents=True, exist_ok=True)

                with open(search_results_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "query": args.s,
                            "timestamp": time.time(),
                            "total": len(papers),
                            "results": papers,
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )

                logger.info(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_results_file}")

            else:
                # ç»Ÿè®¡æ¨¡å¼ï¼šè·å–å…¨éƒ¨æ–‡çŒ®çš„PMCIDä¿¡æ¯
                # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„é‚®ç®±/APIå¯†é’¥
                email = args.e or NCBI_EMAIL
                api_key = args.k or NCBI_API_KEY

                counter = PMCIDCounter(
                    email=email, api_key=api_key, source=args.S  # ä¼ é€’æ•°æ®æº
                )

                # æ‰§è¡Œç»Ÿè®¡
                stats = counter.count_pmcid(args.s, limit=args.l)

                # æ ¼å¼åŒ–è¾“å‡º
                if args.format and args.format != "console":
                    formatted_output = StatsFormatter.format(stats, args.format)
                    print(formatted_output)

                    # ä¿å­˜æŠ¥å‘Š
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f"pmcid_stats_{timestamp}"
                    StatsFormatter.save_report(stats, filename, args.format)
                else:
                    # ç®€å•çš„æ§åˆ¶å°è¾“å‡º
                    print("\nğŸ“ˆ PMCIDç»Ÿè®¡ç»“æœ:")
                    print(f"   æŸ¥è¯¢: {stats['query']}")
                    print(f"   æ€»æ–‡çŒ®æ•°: {stats['total']:,} ç¯‡")
                    print(f"   æ£€æŸ¥äº†: {stats['checked']:,} ç¯‡ (ç”±-lå‚æ•°æŒ‡å®š)")
                    print(
                        f"   å…¶ä¸­æœ‰PMCID: {stats['with_pmcid']:,} ç¯‡ ({stats['rate']:.1f}%)"
                    )
                    print(f"   æ— PMCID: {stats['without_pmcid']:,} ç¯‡")
                    print(f"   è€—æ—¶: {stats['elapsed_seconds']:.1f} ç§’")
                    # é¿å…é™¤é›¶é”™è¯¯
                    if stats["elapsed_seconds"] > 0:
                        print(
                            f"   å¤„ç†é€Ÿåº¦: {stats['checked'] / stats['elapsed_seconds']:.1f} ç¯‡/ç§’"
                        )
                    else:
                        print("   å¤„ç†é€Ÿåº¦: N/A (ä½¿ç”¨ç¼“å­˜)")

                    if stats["with_pmcid"] > 0:
                        print("\nğŸ’¾ å¦‚æœä¸‹è½½æ‰€æœ‰å¼€æ”¾è·å–æ–‡çŒ®:")
                        print(f"   æ–‡ä»¶æ•°é‡: {stats['with_pmcid']:,} ä¸ªPDF")
                        size_mb = stats["estimated_size_mb"]
                        size_gb = size_mb / 1024
                        print(f"   ä¼°ç®—å¤§å°: {size_mb:.1f} MB ({size_gb:.2f} GB)")

                    # å¦‚æœæ£€æŸ¥çš„æ ·æœ¬æ•°å°äºæ€»æ•°ï¼Œæä¾›è¯´æ˜
                    if stats["checked"] < stats["total"]:
                        print(
                            f"\nğŸ“ è¯´æ˜: ä»…æ£€æŸ¥äº†å‰ {stats['checked']:,} ç¯‡æ–‡çŒ®çš„PMCIDçŠ¶æ€"
                        )

                return

            # ä¸‹è½½PDF
            logger.info("\nğŸ“¥ å¼€å§‹ä¸‹è½½PDF...")

            # åªä¸‹è½½æœ‰PMCIDçš„å¼€æ”¾è·å–æ–‡çŒ®
            oa_papers = [p for p in papers if p.get("pmcid")]
            logger.info(f"   æ‰¾åˆ° {len(oa_papers)} ç¯‡å¼€æ”¾è·å–æ–‡çŒ®")

            if oa_papers:
                # ä½¿ç”¨ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨
                download_manager = UnifiedDownloadManager(
                    fetcher=fetcher,
                    max_workers=args.t,
                )
                results = download_manager.download_batch(oa_papers, timeout=TIMEOUT)

                # ç»Ÿè®¡ç»“æœ
                stats = log_download_stats(logger, results)

                # ä¿å­˜ä¸‹è½½ç»“æœ
                if stats["success_count"] > 0:
                    download_results_file = Path(args.o) / "download_results.json"
                    with open(download_results_file, "w", encoding="utf-8") as f:
                        json.dump(
                            {
                                "timestamp": time.time(),
                                "total": stats["total"],
                                "success": stats["success_count"],
                                "results": results,
                            },
                            f,
                            indent=2,
                            ensure_ascii=False,
                        )

                    logger.info(f"\nğŸ’¾ ä¸‹è½½ç»“æœå·²ä¿å­˜åˆ°: {download_results_file}")

        elif args.m:
            # ç»Ÿä¸€æ‰¹é‡è¾“å…¥ï¼ˆCSVæ–‡ä»¶/å•ä¸ªæ ‡è¯†ç¬¦/é€—å·åˆ†éš”åˆ—è¡¨ï¼‰
            logger.info(f"\nğŸ“‹ æ‰¹é‡è¾“å…¥ä¸‹è½½: {args.m}")

            # ä½¿ç”¨ç»Ÿä¸€è¾“å…¥ä¸‹è½½æ–¹æ³•
            results = fetcher.download_from_unified_input(
                input_value=args.m, column=args.c, limit=args.l, max_workers=args.t
            )

            # ç»Ÿè®¡ç»“æœ
            stats = log_download_stats(logger, results)

            # ä¿å­˜ä¸‹è½½ç»“æœ
            if stats["success_count"] > 0:
                download_results_file = Path(args.o) / "download_results.json"
                download_results_file.parent.mkdir(parents=True, exist_ok=True)

                with open(download_results_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "timestamp": time.time(),
                            "source": "unified_input",
                            "input_value": args.m,
                            "total": stats["total"],
                            "success": stats["success_count"],
                            "results": results,
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )

                logger.info(f"\nğŸ’¾ ä¸‹è½½ç»“æœå·²ä¿å­˜åˆ°: {download_results_file}")

        else:
            # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œå› ä¸ºå‚æ•°æ˜¯required=True
            logger.error("âŒ è¯·æŒ‡å®š -s æˆ– -m å‚æ•°")
            exit(1)

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        exit(1)
    except Exception as e:
        logger.error(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        exit(1)

    logger.info("\nâœ¨ ä¸‹è½½å®Œæˆ")
    exit(0)


if __name__ == "__main__":
    main()
