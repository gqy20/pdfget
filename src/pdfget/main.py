#!/usr/bin/env python3
"""
PDFä¸‹è½½å™¨ä¸»ç¨‹åº
ç‹¬ç«‹çš„æ–‡çŒ®PDFä¸‹è½½å·¥å…·
"""

import argparse
import json
import sys
import time
from pathlib import Path

import logging

from .fetcher import PaperFetcher
from .config import TIMEOUT, MAX_RETRIES, DELAY, OUTPUT_DIR, LOG_LEVEL, LOG_FORMAT


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ–‡çŒ®ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¸‹è½½å•ä¸ªæ–‡çŒ®
  python main.py --doi 10.1016/j.cell.2020.01.021

  # æœç´¢æ–‡çŒ®
  python main.py --search "machine learning cancer"
  python main.py -s "title:deep learning AND author:hinton" --limit 20

  # é«˜çº§æ£€ç´¢
  python main.py --search '"quantum computing" AND NOT review' --download
  python main.py -s "cancer AND immunotherapy" -d --limit 30

  # æ‰¹é‡ä¸‹è½½ï¼ˆä»CSVæ–‡ä»¶ï¼‰
  python main.py --input dois.csv --column doi

  # æ‰¹é‡ä¸‹è½½ï¼ˆä»æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªDOIï¼‰
  python main.py --input dois.txt

  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python main.py --doi 10.1016/j.cell.2020.01.021 --output ./my_pdfs

  # å¯ç”¨è¯¦ç»†æ—¥å¿—
  python main.py --doi 10.1016/j.cell.2020.01.021 --verbose
        """
    )

    # è¾“å…¥é€‰é¡¹
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--doi", help="å•ä¸ªDOI")
    group.add_argument("--input", "-i", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆCSVæˆ–TXTï¼‰")
    group.add_argument("--search", "-s", help="æœç´¢æ–‡çŒ®ï¼ˆæ”¯æŒé«˜çº§æ£€ç´¢è¯­æ³•ï¼‰")

    # å¯é€‰å‚æ•°
    parser.add_argument("--column", "-c", default="doi",
                       help="CSVæ–‡ä»¶ä¸­çš„DOIåˆ—åï¼ˆé»˜è®¤: doiï¼‰")
    parser.add_argument("--output", "-o", default="data/pdfs",
                       help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/pdfsï¼‰")
    parser.add_argument("--cache", default="data/cache",
                       help="ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤: data/cacheï¼‰")
    parser.add_argument("--delay", type=float, default=DELAY,
                       help=f"è¯·æ±‚é—´å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: {DELAY}ï¼‰")
    parser.add_argument("--timeout", type=int, default=TIMEOUT,
                       help=f"è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤: {TIMEOUT}ç§’ï¼‰")
    parser.add_argument("--limit", "-l", type=int, default=50,
                       help="æœç´¢ç»“æœæ•°é‡é™åˆ¶ï¼ˆé»˜è®¤: 50ï¼‰")
    parser.add_argument("--download", "-d", action="store_true",
                       help="æœç´¢åè‡ªåŠ¨ä¸‹è½½PDF")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else LOG_LEVEL,
        format=LOG_FORMAT
    )
    logger = logging.getLogger("PDFDownloader")

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    fetcher = PaperFetcher(cache_dir=args.cache)

    logger.info("ğŸš€ PDFä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"   è¾“å‡ºç›®å½•: {args.output}")
    logger.info(f"   ç¼“å­˜ç›®å½•: {args.cache}")
    logger.info(f"   è¯·æ±‚å»¶è¿Ÿ: {args.delay}ç§’")

    try:
        if args.doi:
            # å•ä¸ªDOIä¸‹è½½
            logger.info(f"\nğŸ“„ ä¸‹è½½å•ä¸ªæ–‡çŒ®: {args.doi}")
            result = fetcher.fetch_by_doi(args.doi, timeout=args.timeout)

            if result.get("success"):
                logger.info("âœ… ä¸‹è½½æˆåŠŸ!")
                if result.get("pdf_path"):
                    logger.info(f"   PDFè·¯å¾„: {result['pdf_path']}")
                else:
                    logger.info(f"   HTMLé“¾æ¥: {result.get('full_text_url')}")
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")

        elif args.search:
            # æœç´¢æ–‡çŒ®
            logger.info(f"\nğŸ” æœç´¢æ–‡çŒ®: {args.search}")
            papers = fetcher.search_papers(args.search, limit=args.limit)

            if not papers:
                logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                return 1

            # æ˜¾ç¤ºæœç´¢ç»“æœ
            logger.info(f"\nğŸ“Š æœç´¢ç»“æœ ({len(papers)} ç¯‡):")
            for i, paper in enumerate(papers, 1):
                logger.info(f"\n{i}. {paper['title']}")
                logger.info(f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
                logger.info(f"   æœŸåˆŠ: {paper['journal']} ({paper['year']})")
                if paper['doi']:
                    logger.info(f"   DOI: {paper['doi']}")
                logger.info(f"   å¼€æ”¾è·å–: {'æ˜¯' if paper['isOpenAccess'] else 'å¦'}")

            # ä¿å­˜æœç´¢ç»“æœ
            search_results_file = Path(args.output) / f"search_results_{int(time.time())}.json"
            search_results_file.parent.mkdir(parents=True, exist_ok=True)

            with open(search_results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "query": args.search,
                    "timestamp": time.time(),
                    "total": len(papers),
                    "results": papers
                }, f, indent=2, ensure_ascii=False)

            logger.info(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_results_file}")

            # å¦‚æœéœ€è¦ä¸‹è½½PDF
            if args.download:
                logger.info(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½PDF...")

                # åªä¸‹è½½æœ‰PMCIDçš„å¼€æ”¾è·å–æ–‡çŒ®
                oa_papers = [p for p in papers if p['pmcid']]
                logger.info(f"   æ‰¾åˆ° {len(oa_papers)} ç¯‡å¼€æ”¾è·å–æ–‡çŒ®")

                if oa_papers:
                    # æ„é€ DOIåˆ—è¡¨
                    dois = [p['doi'] for p in oa_papers if p['doi']]

                    if dois:
                        # æ‰¹é‡ä¸‹è½½
                        results = fetcher.fetch_batch(dois, delay=args.delay)

                        # ç»Ÿè®¡ç»“æœ
                        success_count = sum(1 for r in results if r.get("success"))
                        pdf_count = sum(1 for r in results if r.get("pdf_path"))
                        html_count = sum(1 for r in results if r.get("full_text_url"))

                        logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
                        logger.info(f"   æ€»è®¡: {len(results)}")
                        logger.info(f"   æˆåŠŸ: {success_count}")
                        logger.info(f"   PDF: {pdf_count}")
                        logger.info(f"   HTML: {html_count}")
                        logger.info(f"   å¤±è´¥: {len(results) - success_count}")

                        # ä¿å­˜ä¸‹è½½ç»“æœ
                        if success_count > 0:
                            download_results_file = Path(args.output) / "download_results.json"
                            with open(download_results_file, 'w', encoding='utf-8') as f:
                                json.dump({
                                    "timestamp": time.time(),
                                    "total": len(results),
                                    "success": success_count,
                                    "results": results
                                }, f, indent=2, ensure_ascii=False)

                            logger.info(f"\nğŸ’¾ ä¸‹è½½ç»“æœå·²ä¿å­˜åˆ°: {download_results_file}")

        else:
            # æ‰¹é‡ä¸‹è½½
            logger.info(f"\nğŸ“š æ‰¹é‡ä¸‹è½½: {args.input}")

            # è¯»å–DOIåˆ—è¡¨
            input_path = Path(args.input)
            if not input_path.exists():
                logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
                return 1

            if input_path.suffix.lower() == '.csv':
                # è¯»å–CSVæ–‡ä»¶
                import pandas as pd
                try:
                    df = pd.read_csv(input_path)
                    if args.column not in df.columns:
                        logger.error(f"âŒ CSVæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åˆ—: {args.column}")
                        return 1

                    dois = df[args.column].dropna().unique().tolist()
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªå”¯ä¸€DOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
                    return 1

            else:
                # è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªDOIï¼‰
                try:
                    with open(input_path, 'r') as f:
                        dois = [line.strip() for line in f if line.strip()]
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªDOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    return 1

            # æ‰¹é‡å¤„ç†
            results = fetcher.fetch_batch(dois, delay=args.delay)

            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if r.get("success"))
            pdf_count = sum(1 for r in results if r.get("pdf_path"))
            html_count = sum(1 for r in results if r.get("full_text_url"))

            logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            logger.info(f"   æ€»è®¡: {len(results)}")
            logger.info(f"   æˆåŠŸ: {success_count}")
            logger.info(f"   PDF: {pdf_count}")
            logger.info(f"   HTML: {html_count}")
            logger.info(f"   å¤±è´¥: {len(results) - success_count}")

            # ä¿å­˜ç»“æœ
            if success_count > 0:
                output_file = Path(args.output) / "download_results.json"
                output_file.parent.mkdir(parents=True, exist_ok=True)

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "timestamp": time.time(),
                        "total": len(results),
                        "success": success_count,
                        "results": results
                    }, f, indent=2, ensure_ascii=False)

                logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        return 130
    except Exception as e:
        logger.error(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return 1

    logger.info("\nâœ¨ ä¸‹è½½å®Œæˆ")
    return 0


if __name__ == "__main__":
    sys.exit(main())