#!/usr/bin/env python3
"""PMCIDç»Ÿè®¡å™¨ - å¹¶è¡Œç»Ÿè®¡å¼€æ”¾è·å–æ–‡çŒ®æ•°é‡"""

import hashlib
import json
import random
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import requests

from . import config
from .config import (
    AVG_PDF_SIZE_MB,
    CACHE_DIR,
    COUNT_BATCH_SIZE,
    COUNT_MAX_WORKERS,
    NCBI_API_KEY,
    NCBI_EMAIL,
    PUBMED_MAX_RESULTS,
)
from .logger import get_logger


class PMCIDCounter:
    """PMCIDç»Ÿè®¡å™¨"""

    def __init__(
        self,
        email: str | None = None,
        api_key: str | None = None,
        cache_dir: str | None = None,
    ):
        """åˆå§‹åŒ–è®¡æ•°å™¨

        Args:
            email: NCBI APIé‚®ç®±ï¼ˆå¯é€‰ï¼‰
            api_key: NCBI APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
            cache_dir: ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„CACHE_DIRï¼‰
        """
        self.email = email or NCBI_EMAIL
        self.api_key = api_key or NCBI_API_KEY
        self.logger = get_logger(__name__)
        self.session = requests.Session()
        # ä½¿ç”¨ä¼ å…¥çš„cache_diræˆ–é…ç½®ä¸­çš„CACHE_DIR
        self.cache_dir = Path(cache_dir) if cache_dir else CACHE_DIR
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update(config.HEADERS)

        # NCBI APIåŸºç¡€URL
        self.ncbi_base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

    def _fetch_batch_pmcid(
        self, batch_pmids: list[str], batch_num: int, total_batches: int
    ) -> tuple[int, int]:
        """è·å–ä¸€æ‰¹PMIDsä¸­æ˜¯å¦æœ‰PMCIDçš„ç»Ÿè®¡

        Args:
            batch_pmids: PMIDsåˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            total_batches: æ€»æ‰¹æ¬¡æ•°

        Returns:
            (æœ‰PMCIDçš„æ–‡çŒ®æ•°, æ€»æ–‡çŒ®æ•°)
        """
        fetch_url = f"{self.ncbi_base_url}efetch.fcgi"

        params = {
            "db": "pubmed",
            "id": ",".join(batch_pmids),
            "retmode": "xml",
            "rettype": "full",
        }

        if self.email:
            params["email"] = self.email
        if self.api_key:
            params["api_key"] = self.api_key

        # éšæœºå»¶è¿Ÿï¼Œé¿å…æ‰€æœ‰çº¿ç¨‹åŒæ—¶è¯·æ±‚
        time.sleep(random.uniform(0.05, 0.15))

        try:
            response = self.session.get(
                fetch_url, params=params, timeout=config.TIMEOUT
            )
            response.raise_for_status()
            xml = response.text

            # æŒ‰ PubmedArticle åˆ†å‰²
            article_pattern = r"<PubmedArticle>(.*?)</PubmedArticle>"
            articles = re.findall(article_pattern, xml, re.DOTALL)

            # ç»Ÿè®¡è¿™æ‰¹ä¸­æœ‰å¤šå°‘æ–‡ç« æœ‰PMCID
            batch_with_pmcid = sum(
                1 for article in articles if '<ArticleId IdType="pmc">' in article
            )

            self.logger.debug(
                f"æ‰¹æ¬¡ {batch_num:2d}/{total_batches} - æœ‰PMCID: {batch_with_pmcid:3d}/{len(articles):3d}"
            )

            return batch_with_pmcid, len(articles)

        except Exception as e:
            self.logger.warning(
                f"æ‰¹æ¬¡ {batch_num:2d}/{total_batches} é”™è¯¯: {str(e)[:50]}..."
            )
            return 0, len(batch_pmids)

    def _get_cache_file(self, query: str, source: str = "pubmed") -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        content = f"{query}:{source}".encode()
        hash_key = hashlib.md5(content).hexdigest()
        return self.cache_dir / f"search_{hash_key}.json"

    def _load_cache(self, query: str) -> list[dict] | None:
        """åŠ è½½ PaperFetcher çš„æœç´¢ç¼“å­˜"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æº
        sources = ["pubmed", "europe_pmc"]
        for source in sources:
            cache_file = self._get_cache_file(query, source)
            if cache_file.exists():
                try:
                    with open(cache_file, encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, list) and data:
                            self.logger.info(f"ä» {source} ç¼“å­˜åŠ è½½ {len(data)} æ¡ç»“æœ")
                            return data
                except Exception as e:
                    self.logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥ {cache_file}: {str(e)}")

        return None

    def _statistics_from_cache(self, papers: list[dict]) -> dict:
        """ä»ç¼“å­˜çš„æ–‡çŒ®åˆ—è¡¨ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        total = len(papers)
        with_pmcid = sum(1 for p in papers if p.get("pmcid"))
        without_pmcid = total - with_pmcid
        rate = (with_pmcid / total) * 100 if total > 0 else 0

        # ä¼°ç®—æ€»æ–‡çŒ®æ•°ï¼ˆå¦‚æœæœ‰æ›´å¤šä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨ï¼‰
        total_available = total  # ç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ä»æœç´¢APIè·å–

        return {
            "query": getattr(self, "_current_query", ""),
            "total": total_available,
            "checked": total,
            "with_pmcid": with_pmcid,
            "without_pmcid": without_pmcid,
            "rate": rate,
            "estimated_size_mb": with_pmcid * AVG_PDF_SIZE_MB,
            "elapsed_seconds": 0,  # ä»ç¼“å­˜åŠ è½½ï¼Œè€—æ—¶ä¸º0
            "processing_speed": 0.0,  # ä»ç¼“å­˜åŠ è½½ï¼Œé€Ÿåº¦è®¾ä¸º0
            "from_cache": True,
        }

    def _rate_limit(self) -> None:
        """PubMed APIé€Ÿç‡é™åˆ¶"""
        # å…è´¹ç”¨æˆ·ï¼š3è¯·æ±‚/ç§’
        # æœ‰APIå¯†é’¥ï¼š10è¯·æ±‚/ç§’
        if self.api_key:
            time.sleep(0.1)  # 10è¯·æ±‚/ç§’
        else:
            time.sleep(0.34)  # çº¦3è¯·æ±‚/ç§’

    def count_pmcid(
        self,
        query: str,
        limit: int = 5000,
        use_cache: bool = True,
        trigger_search: bool = True,
    ) -> dict:
        """ç»Ÿè®¡æŸ¥è¯¢ç»“æœä¸­æœ‰PMCIDçš„æ–‡çŒ®æ•°é‡

        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: æœ€å¤§ç»“æœæ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            trigger_search: å¦‚æœæ²¡æœ‰ç¼“å­˜æ˜¯å¦è§¦å‘æœç´¢åˆ›å»ºç¼“å­˜

        Returns:
            ç»Ÿè®¡ç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ” ç»Ÿè®¡PMCID: {query}")
        self._current_query = query

        # 1. é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_papers = self._load_cache(query)
            if cached_papers:
                self.logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®ç”Ÿæˆç»Ÿè®¡")
                return self._statistics_from_cache(cached_papers)

        # 2. å¦‚æœæ²¡æœ‰ç¼“å­˜ä¸”ä¸è§¦å‘æœç´¢ï¼ŒåªåšåŸºæœ¬ç»Ÿè®¡
        if not trigger_search:
            self.logger.info("ğŸ“Š æ‰§è¡ŒåŸºæœ¬ç»Ÿè®¡ï¼ˆä¸åˆ›å»ºç¼“å­˜ï¼‰")
            return self._count_without_cache(query, limit)

        # 3. è§¦å‘æœç´¢ä»¥åˆ›å»ºç¼“å­˜
        self.logger.info("ğŸ“¥ æ— ç¼“å­˜ï¼Œè§¦å‘æœç´¢ä»¥ç”Ÿæˆç¼“å­˜...")
        try:
            # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from .fetcher import PaperFetcher

            fetcher = PaperFetcher(
                cache_dir=str(self.cache_dir), default_source="pubmed"
            )

            # æœç´¢å¹¶ç¼“å­˜ç»“æœ
            papers = fetcher.search_papers(query, limit=limit, fetch_pmcid=True)

            if papers:
                self.logger.info(f"âœ… æœç´¢å¹¶ç¼“å­˜äº† {len(papers)} ç¯‡æ–‡çŒ®")
                return self._statistics_from_cache(papers)
            else:
                self.logger.warning("âš  æœªæ‰¾åˆ°æ–‡çŒ®")
                return self._count_without_cache(query, limit)

        except Exception as e:
            self.logger.error(f"è§¦å‘æœç´¢å¤±è´¥: {str(e)}")
            self.logger.info("ğŸ“Š å›é€€åˆ°åŸºæœ¬ç»Ÿè®¡æ¨¡å¼")
            return self._count_without_cache(query, limit)

    def _count_without_cache(self, query: str, limit: int = 5000) -> dict:
        """ä¸ä½¿ç”¨ç¼“å­˜çš„åŸå§‹ç»Ÿè®¡æ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        # 1. è·å–PMIDåˆ—è¡¨
        search_url = f"{self.ncbi_base_url}esearch.fcgi"
        search_params: dict[str, str | int] = {
            "db": "pubmed",
            "term": query,
            "retmode": "json",
            "retmax": min(limit, PUBMED_MAX_RESULTS),  # PubMedå•æ¬¡æœ€å¤šè¿”å›10000æ¡
        }

        if self.email:
            search_params["email"] = self.email
        if self.api_key:
            search_params["api_key"] = self.api_key

        response = self.session.get(
            search_url,
            params=search_params,
            timeout=config.TIMEOUT,  # type: ignore[arg-type]
        )
        response.raise_for_status()

        search_data = response.json()
        pmids = search_data.get("esearchresult", {}).get("idlist", [])
        total_available = int(search_data.get("esearchresult", {}).get("count", 0))

        self.logger.info(f"ğŸ“Š æ€»æ–‡çŒ®æ•°: {total_available}")
        self.logger.info(f"   è·å–çš„PMIDæ•°: {len(pmids)}")

        if not pmids:
            return {
                "query": query,
                "total": 0,
                "checked": 0,
                "with_pmcid": 0,
                "without_pmcid": 0,
                "rate": 0.0,
                "estimated_size_mb": 0,
                "elapsed_seconds": 0,
            }

        # 2. åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        batch_size = COUNT_BATCH_SIZE
        max_workers = COUNT_MAX_WORKERS
        batches = [pmids[i : i + batch_size] for i in range(0, len(pmids), batch_size)]

        self.logger.info(
            f"ğŸš€ ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œå…± {len(batches)} æ‰¹ï¼Œæ¯æ‰¹ {batch_size} ä¸ªPMID"
        )
        self.logger.info(f"   ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†")

        start_time = time.time()
        total_with_pmcid = 0
        total_checked = 0

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_batch = {
                executor.submit(self._fetch_batch_pmcid, batch, i + 1, len(batches)): i
                + 1
                for i, batch in enumerate(batches)
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_batch):
                batch_num = future_to_batch[future]
                try:
                    batch_count, batch_articles = future.result()
                    total_with_pmcid += batch_count
                    total_checked += batch_articles
                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¼‚å¸¸: {e}")

        elapsed = time.time() - start_time

        # 3. è®¡ç®—ç»“æœ
        rate = (total_with_pmcid / total_checked) * 100 if total_checked > 0 else 0
        avg_pdf_size = AVG_PDF_SIZE_MB  # MB
        estimated_size_mb = total_with_pmcid * avg_pdf_size

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        return {
            "query": query,
            "total": total_available,
            "checked": total_checked,
            "with_pmcid": total_with_pmcid,
            "without_pmcid": total_checked - total_with_pmcid,
            "rate": rate,
            "estimated_size_mb": estimated_size_mb,
            "elapsed_seconds": elapsed,
            "processing_speed": total_checked / elapsed if elapsed > 0 else 0,
        }
