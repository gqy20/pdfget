#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–‡çŒ®è·å–å™¨ - Linusé£æ ¼
åªåšä¸€ä»¶äº‹ï¼šä¸‹è½½å¼€æ”¾è·å–æ–‡çŒ®
éµå¾ªKISSåŸåˆ™ï¼šKeep It Simple, Stupid
"""

import hashlib
import json
import re
import time
from pathlib import Path
from urllib.parse import quote

import requests

import logging


class PaperFetcher:
    """ç®€å•æ–‡çŒ®è·å–å™¨"""

    def __init__(
        self,
        cache_dir: str = "data/cache",
        output_dir: str = "data/pdfs",
        default_source: str = "pubmed",
        sources: list[str] | None = None,
    ):
        """
        åˆå§‹åŒ–è·å–å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            output_dir: PDFè¾“å‡ºç›®å½•
            default_source: é»˜è®¤æ•°æ®æº (pubmed, europe_pmc)
            sources: æ”¯æŒçš„æ•°æ®æºåˆ—è¡¨
        """
        self.logger = logging.getLogger("PaperFetcher")
        self.cache_dir = Path(cache_dir)
        self.output_dir = Path(output_dir)
        self.default_source = default_source
        self.sources = sources or ["pubmed", "europe_pmc"]

        # NCBI é…ç½®
        self.ncbi_base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = ""  # å¯é…ç½®é‚®ç®±ä»¥æé«˜è¯·æ±‚é™åˆ¶
        self.api_key = ""  # å¯é€‰ API å¯†é’¥
        self.rate_limit = 3  # æ¯ç§’æœ€å¤š3æ¬¡è¯·æ±‚
        self._last_request_time = 0.0

        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç®€å•çš„HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {"User-Agent": "Mozilla/5.0 (compatible; PDFGet/1.0)"}
        )

    def parse_query(self, query: str) -> str:
        """
        è§£æé«˜çº§æ£€ç´¢è¯ä¸ºEurope PMCæ ¼å¼

        æ”¯æŒçš„è¯­æ³•ï¼š
        - å¸ƒå°”è¿ç®—ç¬¦ï¼šAND, OR, NOT
        - å­—æ®µæ£€ç´¢ï¼štitle:, author:, journal:
        - çŸ­è¯­æ£€ç´¢ï¼š"exact phrase"

        Args:
            query: ç”¨æˆ·è¾“å…¥çš„æ£€ç´¢è¯

        Returns:
            Europe PMCæ ¼å¼çš„æ£€ç´¢è¯
        """
        # å¤„ç†çŸ­è¯­æ£€ç´¢ï¼ˆå¼•å·åŒ…å›´çš„å†…å®¹ï¼‰
        phrase_pattern = r'"([^"]+)"'
        phrases = re.findall(phrase_pattern, query)

        # ä¸´æ—¶æ›¿æ¢çŸ­è¯­ä¸ºå ä½ç¬¦
        for i, phrase in enumerate(phrases):
            query = query.replace(f'"{phrase}"', f"__PHRASE_{i}__")

        # å¤„ç†å­—æ®µæ£€ç´¢
        field_mappings = {
            "title:": "TITLE:",
            "author:": "AUTHOR:",
            "journal:": "JOURNAL:",
            "abstract:": "ABSTRACT:",
        }

        for user_field, pmc_field in field_mappings.items():
            query = query.replace(user_field, pmc_field)

        # æ¢å¤çŸ­è¯­ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å¼•å·
        for i, phrase in enumerate(phrases):
            query = query.replace(f"__PHRASE_{i}__", f'"{phrase}"')

        # å¤„ç†å¸ƒå°”è¿ç®—ç¬¦ï¼ˆç¡®ä¿å¤§å†™ï¼‰
        query = (
            query.replace(" and ", " AND ")
            .replace(" or ", " OR ")
            .replace(" not ", " NOT ")
        )

        return query.strip()

    def parse_query_pubmed(self, query: str) -> str:
        """
        è§£æé«˜çº§æ£€ç´¢è¯ä¸º PubMed æ ¼å¼

        æ”¯æŒçš„è¯­æ³•ï¼š
        - å¸ƒå°”è¿ç®—ç¬¦ï¼šAND, OR, NOT
        - å­—æ®µæ£€ç´¢ï¼štitle, author, journal, abstract, year, mesh
        - çŸ­è¯­æ£€ç´¢ï¼š"exact phrase"

        Args:
            query: ç”¨æˆ·è¾“å…¥çš„æ£€ç´¢è¯

        Returns:
            PubMed æ ¼å¼çš„æ£€ç´¢è¯
        """
        # å¤„ç†çŸ­è¯­æ£€ç´¢ï¼ˆå¼•å·åŒ…å›´çš„å†…å®¹ï¼‰
        phrase_pattern = r'"([^"]+)"'
        phrases = re.findall(phrase_pattern, query)

        # ä¸´æ—¶æ›¿æ¢çŸ­è¯­ä¸ºå ä½ç¬¦
        for i, phrase in enumerate(phrases):
            query = query.replace(f'"{phrase}"', f"__PHRASE_{i}__")

        # å¤„ç†å­—æ®µæ£€ç´¢ï¼ˆPubMed æ ¼å¼ï¼‰
        field_mappings = {
            "title:": "[Title]",
            "author:": "[Author]",
            "journal:": "[Journal]",
            "abstract:": "[Abstract]",
            "year:": "[Date - Publication]",
            "mesh:": "[MeSH Terms]",
        }

        for user_field, pubmed_field in field_mappings.items():
            query = query.replace(user_field, pubmed_field)

        # æ¢å¤çŸ­è¯­ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å¼•å·
        for i, phrase in enumerate(phrases):
            query = query.replace(f"__PHRASE_{i}__", f'"{phrase}"')

        # å¤„ç†å¸ƒå°”è¿ç®—ç¬¦ï¼ˆPubMed å¤§å°å†™æ•æ„Ÿï¼‰
        query = (
            query.replace(" and ", " AND ")
            .replace(" or ", " OR ")
            .replace(" not ", " NOT ")
        )

        return query.strip()

    def _rate_limit_pubmed(self) -> None:
        """å¤„ç† PubMed API è¯·æ±‚é¢‘ç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self._last_request_time

        if time_since_last < (1.0 / self.rate_limit):
            time.sleep((1.0 / self.rate_limit) - time_since_last)

        self._last_request_time = time.time()

    def search_papers(
        self, query: str, limit: int = 50, source: str | None = None
    ) -> list[dict]:
        """
        é€šè¿‡æŒ‡å®šæ•°æ®æºæœç´¢æ–‡çŒ®

        Args:
            query: æ£€ç´¢è¯ï¼ˆæ”¯æŒé«˜çº§è¯­æ³•ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            source: æ•°æ®æº (pubmed, europe_pmc, both)

        Returns:
            æ–‡çŒ®åˆ—è¡¨ï¼ŒåŒ…å«DOIã€æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        # ç¡®å®šæ•°æ®æº
        source = source or self.default_source

        if source == "pubmed":
            return self.search_pubmed(query, limit)
        elif source == "europe_pmc":
            return self.search_europe_pmc(query, limit)
        elif source == "both":
            return self.search_both_sources(query, limit)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source}")

    def search_europe_pmc(self, query: str, limit: int = 50) -> list[dict]:
        """
        é€šè¿‡Europe PMCæœç´¢æ–‡çŒ®

        Args:
            query: æ£€ç´¢è¯ï¼ˆæ”¯æŒé«˜çº§è¯­æ³•ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            æ–‡çŒ®åˆ—è¡¨ï¼ŒåŒ…å«DOIã€æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        self.logger.info(f"ğŸ” æœç´¢æ–‡çŒ® (Europe PMC): {query}")

        # è§£ææ£€ç´¢è¯
        parsed_query = self.parse_query(query)
        self.logger.debug(f"  è§£æå: {parsed_query}")

        # æ„å»ºæœç´¢URL
        search_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
        params = {
            "query": parsed_query,
            "resulttype": "core",
            "format": "json",
            "pageSize": min(limit, 1000),  # Europe PMCé™åˆ¶æœ€å¤š1000æ¡
            "cursorMark": "*",
        }

        try:
            response = self.session.get(search_url, params=params, timeout=30)  # type: ignore[arg-type]
            response.raise_for_status()

            data = response.json()

            if data.get("hitCount", 0) == 0:
                self.logger.info("  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                return []

            # å¤„ç†ç»“æœ
            papers = []
            results = data.get("resultList", {}).get("result", [])

            for i, record in enumerate(results[:limit]):
                # è·å–æœŸåˆŠä¿¡æ¯
                journal_info = record.get("journalInfo", {})

                paper = {
                    "title": record.get("title", ""),
                    "authors": [
                        a.strip() for a in record.get("authorString", "").split(",")
                    ]
                    if record.get("authorString")
                    else [],
                    "journal": journal_info.get("journal", {}).get("title", ""),
                    "year": record.get("pubYear", ""),
                    "doi": record.get("doi", ""),
                    "pmcid": record.get("pmcid", ""),
                    "pmid": record.get("pmid", ""),
                    "abstract": record.get("abstractText", ""),
                    "isOpenAccess": bool(
                        record.get("pmcid")
                    ),  # æœ‰PMCIDé€šå¸¸è¡¨ç¤ºå¼€æ”¾è·å–
                    "source": "Europe PMC",
                    # æ–°å¢çš„10ä¸ªå­—æ®µ
                    "affiliation": record.get("affiliation", ""),
                    "volume": journal_info.get("volume", ""),
                    "issue": journal_info.get("issue", ""),
                    "pages": record.get("pageInfo", ""),
                    "license": record.get("license", ""),
                    "citedBy": record.get("citedByCount", 0),
                    "keywords": record.get("keywordList", []),
                    "meshTerms": record.get("meshHeadingList", []),
                    "grants": record.get("grantsList", []),
                    "hasData": record.get("hasData") == "Y",
                    "hasSuppl": record.get("hasSuppl") == "Y",
                }
                papers.append(paper)

                self.logger.info(
                    f"  ğŸ“„ {i + 1}/{min(len(results), limit)}: {paper['title'][:60]}..."
                )

            self.logger.info(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")
            return papers

        except requests.exceptions.Timeout:
            self.logger.error("  âŒ æœç´¢è¶…æ—¶")
            return []
        except requests.exceptions.ConnectionError:
            self.logger.error("  âŒ è¿æ¥å¤±è´¥")
            return []
        except Exception as e:
            self.logger.error(f"  âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []

    def search_pubmed(self, query: str, limit: int = 50) -> list[dict]:
        """
        é€šè¿‡NCBI PubMedæœç´¢æ–‡çŒ®

        Args:
            query: æ£€ç´¢è¯ï¼ˆæ”¯æŒé«˜çº§è¯­æ³•ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            æ–‡çŒ®åˆ—è¡¨ï¼ŒåŒ…å«DOIã€æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        self.logger.info(f"ğŸ” æœç´¢æ–‡çŒ® (PubMed): {query}")

        # è§£ææ£€ç´¢è¯
        parsed_query = self.parse_query_pubmed(query)
        self.logger.debug(f"  è§£æå: {parsed_query}")

        try:
            # 1. ä½¿ç”¨ ESearch è·å– PMIDs
            self._rate_limit_pubmed()
            search_url = f"{self.ncbi_base_url}esearch.fcgi"
            search_params: dict[str, str | int] = {
                "db": "pubmed",
                "term": parsed_query,
                "retmode": "json",
                "retmax": limit,
            }

            if self.email:
                search_params["email"] = self.email
            if self.api_key:
                search_params["api_key"] = self.api_key

            response = self.session.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()

            data = response.json()
            idlist = data.get("esearchresult", {}).get("idlist", [])

            if not idlist:
                self.logger.info("  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                return []

            self.logger.info(f"  æ‰¾åˆ° {len(idlist)} ç¯‡æ–‡çŒ®")

            # 2. ä½¿ç”¨ ESummary è·å–æ–‡çŒ®è¯¦æƒ…
            self._rate_limit_pubmed()
            summary_url = f"{self.ncbi_base_url}esummary.fcgi"
            summary_params = {
                "db": "pubmed",
                "id": ",".join(idlist),
                "retmode": "json",
            }

            if self.email:
                summary_params["email"] = self.email
            if self.api_key:
                summary_params["api_key"] = self.api_key

            response = self.session.get(summary_url, params=summary_params, timeout=30)
            response.raise_for_status()

            summary_data = response.json()
            result_data = summary_data.get("result", {})

            # 3. ä½¿ç”¨ EFetch è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ PMCIDï¼‰
            self._rate_limit_pubmed()
            fetch_url = f"{self.ncbi_base_url}efetch.fcgi"
            fetch_params = {
                "db": "pubmed",
                "id": ",".join(idlist),
                "retmode": "xml",
                "rettype": "full",
            }

            if self.email:
                fetch_params["email"] = self.email
            if self.api_key:
                fetch_params["api_key"] = self.api_key

            try:
                response = self.session.get(fetch_url, params=fetch_params, timeout=30)
                response.raise_for_status()
                xml_data = response.text
            except Exception:
                xml_data = ""

            # è§£æ XML è·å– PMCID
            import re

            pmid_to_pmcid = {}
            if xml_data:
                # æŸ¥æ‰¾ PMCID ä¿¡æ¯
                pmcid_pattern = r'<ArticleId IdType="pmc">([^<]+)</ArticleId>'
                pmid_pattern = r"<PMID.*?>(\d+)</PMID>"

                current_pmid = None
                for line in xml_data.split("\n"):
                    pmid_match = re.search(pmid_pattern, line)
                    if pmid_match:
                        current_pmid = pmid_match.group(1)

                    pmcid_match = re.search(pmcid_pattern, line)
                    if pmcid_match and current_pmid:
                        pmid_to_pmcid[current_pmid] = pmcid_match.group(1)

            # å¤„ç†ç»“æœ
            papers = []
            for i, pmid in enumerate(idlist[:limit]):
                if pmid not in result_data:
                    continue

                record = result_data[pmid]

                # æå– DOI
                doi = ""
                if "elocationid" in record:
                    # PubMed ä¸­çš„ DOI æ ¼å¼é€šå¸¸æ˜¯ "doi: 10.xxxx/xxxxx"
                    doi_text = record["elocationid"]
                    if "doi:" in doi_text.lower():
                        doi = doi_text.split("doi:")[-1].strip()

                # æå–ä½œè€…
                authors = []
                if "authors" in record:
                    authors = [author.get("name", "") for author in record["authors"]]

                # æå–å¹´ä»½
                year = ""
                if "pubdate" in record:
                    # PubMed çš„ pubdate æ ¼å¼é€šå¸¸æ˜¯ "2023 Jan" æˆ– "2023 Jan 15"
                    year = record["pubdate"].split()[0]

                # åˆ¤æ–­æ˜¯å¦å¼€æ”¾è·å–ï¼ˆå¦‚æœæœ‰ PMC IDï¼‰
                pmcid = pmid_to_pmcid.get(pmid, record.get("pmcid", ""))
                is_open_access = bool(pmcid)

                paper = {
                    "title": record.get("title", ""),
                    "authors": authors,
                    "journal": record.get("source", ""),
                    "year": year,
                    "doi": doi,
                    "pmcid": pmcid,
                    "pmid": pmid,
                    "abstract": record.get("abstract", ""),
                    "isOpenAccess": is_open_access,
                    "source": "PubMed",
                    # ä¸ºäº†ç»Ÿä¸€æ ¼å¼ï¼Œæ·»åŠ å…¶ä»–å­—æ®µ
                    "affiliation": "",
                    "volume": record.get("volume", ""),
                    "issue": record.get("issue", ""),
                    "pages": record.get("pages", ""),
                    "license": "",
                    "citedBy": 0,
                    "keywords": [],
                    "meshTerms": record.get("meshheadinglist", []),
                    "grants": [],
                    "hasData": False,
                    "hasSuppl": False,
                }
                papers.append(paper)

                self.logger.info(
                    f"  ğŸ“„ {i + 1}/{min(len(idlist), limit)}: {paper['title'][:60]}..."
                )

            self.logger.info(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")
            return papers

        except requests.exceptions.Timeout:
            self.logger.error("  âŒ æœç´¢è¶…æ—¶")
            return []
        except requests.exceptions.ConnectionError:
            self.logger.error("  âŒ è¿æ¥å¤±è´¥")
            return []
        except Exception as e:
            self.logger.error(f"  âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []

    def search_both_sources(self, query: str, limit: int = 50) -> list[dict]:
        """
        åŒæ—¶æœç´¢ä¸¤ä¸ªæ•°æ®æºå¹¶åˆå¹¶ç»“æœ

        Args:
            query: æ£€ç´¢è¯
            limit: æ¯ä¸ªæ•°æ®æºè¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            å»é‡åçš„æ–‡çŒ®åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” æœç´¢æ–‡çŒ® (ä¸¤ä¸ªæ•°æ®æº): {query}")

        all_papers = []

        # å¹¶è¡Œæœç´¢ä¸¤ä¸ªæ•°æ®æº
        pubmed_limit = limit // 2
        europe_limit = limit - pubmed_limit

        # æœç´¢ PubMed
        try:
            pubmed_papers = self.search_pubmed(query, pubmed_limit)
            all_papers.extend(pubmed_papers)
        except Exception as e:
            self.logger.warning(f"PubMed æœç´¢å¤±è´¥: {e}")

        # æœç´¢ Europe PMC
        try:
            europe_papers = self.search_europe_pmc(query, europe_limit)
            all_papers.extend(europe_papers)
        except Exception as e:
            self.logger.warning(f"Europe PMC æœç´¢å¤±è´¥: {e}")

        # å»é‡
        deduplicated = self._deduplicate_papers(all_papers)

        # å¦‚æœè¶…è¿‡é™åˆ¶ï¼ŒæŒ‰æ•°æ®æºä¼˜å…ˆçº§æ’åº
        if len(deduplicated) > limit:
            deduplicated = self._deduplicate_with_priority(deduplicated)[:limit]

        self.logger.info(f"  âœ… æ‰¾åˆ° {len(deduplicated)} ç¯‡æ–‡çŒ®ï¼ˆå»é‡åï¼‰")
        return deduplicated

    def _deduplicate_papers(self, papers: list[dict]) -> list[dict]:
        """æ ¹æ® DOI å»é‡è®ºæ–‡"""
        seen_dois = set()
        deduplicated = []

        for paper in papers:
            doi = paper.get("doi", "")
            if doi and doi not in seen_dois:
                seen_dois.add(doi)
                deduplicated.append(paper)
            elif not doi:
                # æ²¡æœ‰ DOI çš„è®ºæ–‡æ ¹æ® PMID å»é‡
                pmid = paper.get("pmid", "")
                if pmid and pmid not in [p.get("pmid", "") for p in deduplicated]:
                    deduplicated.append(paper)

        return deduplicated

    def _deduplicate_with_priority(self, papers: list[dict]) -> list[dict]:
        """
        æŒ‰æ•°æ®æºä¼˜å…ˆçº§å»é‡ï¼ŒPubMed ä¼˜å…ˆ
        """
        # æŒ‰ DOI åˆ†ç»„
        doi_groups: dict[str, list[dict]] = {}
        for paper in papers:
            doi = paper.get("doi", "")
            if doi:
                if doi not in doi_groups:
                    doi_groups[doi] = []
                doi_groups[doi].append(paper)

        # å¯¹æ¯ä¸ªç»„ï¼Œä¼˜å…ˆé€‰æ‹© PubMed çš„è®ºæ–‡
        deduplicated = []
        seen_dois = set()

        for paper in papers:
            doi = paper.get("doi", "")
            if not doi or doi in seen_dois:
                continue

            # å¦‚æœæœ‰é‡å¤ï¼Œé€‰æ‹© PubMed çš„
            group = doi_groups.get(doi, [paper])
            pubmed_paper = next((p for p in group if p.get("source") == "PubMed"), None)

            selected = pubmed_paper or group[0]
            if selected not in deduplicated:
                deduplicated.append(selected)
                seen_dois.add(doi)

        # æ·»åŠ æ²¡æœ‰ DOI çš„è®ºæ–‡
        for paper in papers:
            if not paper.get("doi") and paper not in deduplicated:
                deduplicated.append(paper)

        return deduplicated

    def _standardize_paper_format(self, paper: dict, source: str) -> dict:
        """æ ‡å‡†åŒ–è®ºæ–‡æ ¼å¼"""
        standardized = {
            "title": paper.get("title", ""),
            "authors": [],
            "journal": "",
            "year": "",
            "doi": "",
            "pmcid": "",
            "pmid": "",
            "abstract": "",
            "isOpenAccess": False,
            "source": source,
            "affiliation": "",
            "volume": "",
            "issue": "",
            "pages": "",
            "license": "",
            "citedBy": 0,
            "keywords": [],
            "meshTerms": [],
            "grants": [],
            "hasData": False,
            "hasSuppl": False,
        }

        if source == "Europe PMC":
            standardized.update(
                {
                    "authors": [
                        a.strip() for a in paper.get("authorString", "").split(",")
                    ]
                    if paper.get("authorString")
                    else [],
                    "journal": paper.get("journalInfo", {})
                    .get("journal", {})
                    .get("title", ""),
                    "doi": paper.get("doi", ""),
                    "pmcid": paper.get("pmcid", ""),
                    "pmid": paper.get("pmid", ""),
                    "abstract": paper.get("abstractText", ""),
                    "isOpenAccess": bool(paper.get("pmcid")),
                    "affiliation": paper.get("affiliation", ""),
                    "volume": paper.get("journalInfo", {}).get("volume", ""),
                    "issue": paper.get("journalInfo", {}).get("issue", ""),
                    "pages": paper.get("pageInfo", ""),
                    "citedBy": paper.get("citedByCount", 0),
                    "keywords": paper.get("keywordList", []),
                    "meshTerms": paper.get("meshHeadingList", []),
                }
            )
        elif source == "PubMed":
            # æå– DOI
            doi = ""
            if "elocationid" in paper and "doi:" in paper["elocationid"].lower():
                doi = paper["elocationid"].split("doi:")[-1].strip()

            # æå–ä½œè€…
            authors = []
            if "authors" in paper:
                authors = [author.get("name", "") for author in paper["authors"]]

            # æå–å¹´ä»½
            year = ""
            if "pubdate" in paper:
                year = paper["pubdate"].split()[0]
                # ä½¿ç”¨å¹´ä»½é¿å…æœªä½¿ç”¨å˜é‡è­¦å‘Š
                standardized["year"] = int(year) if year.isdigit() else 0

            standardized.update(
                {
                    "authors": authors,
                    "journal": paper.get("source", ""),
                    "doi": doi,
                    "pmcid": paper.get("pmcid", ""),
                    "pmid": paper.get("uid", ""),
                    "abstract": paper.get("abstract", ""),
                    "isOpenAccess": bool(paper.get("pmcid")),
                    "meshTerms": paper.get("meshheadinglist", []),
                }
            )

        return standardized

    def search_papers_with_fallback(
        self,
        query: str,
        primary: str = "pubmed",
        fallback: str = "europe_pmc",
        limit: int = 50,
    ) -> list[dict]:
        """
        å¸¦é™çº§çš„æœç´¢ï¼Œä¸»æ•°æ®æºå¤±è´¥æ—¶å°è¯•å¤‡ç”¨æ•°æ®æº
        """
        try:
            return self.search_papers(query, limit, source=primary)
        except Exception as e:
            self.logger.warning(f"{primary} æœç´¢å¤±è´¥ï¼Œå°è¯• {fallback}: {e}")
            try:
                return self.search_papers(query, limit, source=fallback)
            except Exception as e2:
                self.logger.error(f"ä¸¤ä¸ªæ•°æ®æºéƒ½å¤±è´¥: {e2}")
                return []

    def fetch_by_doi(
        self, doi: str, timeout: int = 30, pmcid: str | None = None
    ) -> dict:
        """
        é€šè¿‡DOIè·å–æ–‡çŒ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        ç­–ç•¥ï¼š
        1. å¦‚æœæœ‰PMCIDï¼Œç›´æ¥ä¸‹è½½
        2. å¦åˆ™ä½¿ç”¨ Europe PMC æœç´¢PMCID
        3. å¿«é€Ÿå¤±è´¥ï¼Œä¸é‡è¯•
        4. ç®€å•ç¼“å­˜
        5. ä¸æå¤æ‚çš„ç½‘ç»œç›‘æ§å’Œè‡ªé€‚åº”é‡è¯•

        Args:
            doi: æ–‡çŒ®DOI
            timeout: è¶…æ—¶æ—¶é—´
            pmcid: å¯é€‰çš„PMCIDï¼ˆå¦‚æœå·²çŸ¥ï¼‰

        Returns:
            è·å–ç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ” è·å–æ–‡çŒ®: {doi}")

        # æ£€æŸ¥ç¼“å­˜
        cached_result = self._get_cache(doi)
        if cached_result:
            self.logger.info("  ğŸ“¦ ä»ç¼“å­˜åŠ è½½")
            return cached_result

        # å¦‚æœæœ‰PMCIDï¼Œç›´æ¥å°è¯•ä¸‹è½½
        if pmcid:
            self.logger.info(f"  ğŸ“„ ä½¿ç”¨å·²çŸ¥PMCID: {pmcid}")
            pdf_result = self._download_pdf(pmcid, doi)

            if pdf_result["success"]:
                result = {
                    "success": True,
                    "doi": doi,
                    "pmcid": pmcid,
                    "pdf_path": pdf_result["path"],
                    "content_type": "pdf",
                }
            else:
                # PDFä¸‹è½½å¤±è´¥ï¼Œè¿”å›å…¨æ–‡HTMLé“¾æ¥
                result = {
                    "success": True,
                    "doi": doi,
                    "pmcid": pmcid,
                    "full_text_url": f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/",
                    "content_type": "html",
                }
        else:
            # æ²¡æœ‰PMCIDï¼Œä½¿ç”¨Europe PMCæœç´¢
            result = self._fetch_from_pmc(doi, timeout)

        # ç¼“å­˜ç»“æœ
        self._save_cache(doi, result)

        if result.get("success"):
            self.logger.info("  âœ… è·å–æˆåŠŸ")
        else:
            self.logger.info(f"  âŒ è·å–å¤±è´¥: {result.get('error', 'Unknown error')}")

        return result

    def _fetch_from_pmc(self, doi: str, timeout: int) -> dict:
        """ä»Europe PMCè·å–æ–‡çŒ®"""
        try:
            # æœç´¢PMCID
            search_url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=DOI:{quote(doi)}&resulttype=core&format=json"
            self.logger.debug(f"  ğŸ” Europe PMC URL: {search_url}")

            response = self.session.get(search_url, timeout=timeout)
            response.raise_for_status()

            data = response.json()
            if data.get("hitCount", 0) == 0:
                return {
                    "success": False,
                    "error": "Not found in Europe PMC",
                    "doi": doi,
                }

            record = data["resultList"]["result"][0]
            pmcid = record.get("pmcid")

            if not pmcid:
                self.logger.info("  â­ï¸ æ— PMCIDï¼Œéå¼€æ”¾è·å–æ–‡çŒ®")
                return {
                    "success": False,
                    "error": "Not open access (no PMCID)",
                    "doi": doi,
                }

            self.logger.info(f"  ğŸ“„ æ‰¾åˆ°PMCID: {pmcid}")

            # å°è¯•ä¸‹è½½PDF
            pdf_result = self._download_pdf(pmcid, doi)

            if pdf_result["success"]:
                return {
                    "success": True,
                    "doi": doi,
                    "pmcid": pmcid,
                    "pdf_path": pdf_result["path"],
                    "content_type": "pdf",
                    "title": record.get("title"),
                    "journal": record.get("journalInfo", {})
                    .get("journal", {})
                    .get("title"),
                    "authors": [
                        a.strip() for a in record.get("authorString", "").split(",")
                    ]
                    if record.get("authorString")
                    else [],
                    "year": record.get("pubYear"),
                    "abstract": record.get("abstractText"),
                }

            # PDFä¸‹è½½å¤±è´¥ï¼Œè¿”å›å…¨æ–‡HTMLé“¾æ¥
            return {
                "success": True,
                "doi": doi,
                "pmcid": pmcid,
                "full_text_url": f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/",
                "content_type": "html",
                "title": record.get("title"),
                "authors": [
                    a.strip() for a in record.get("authorString", "").split(",")
                ]
                if record.get("authorString")
                else [],
                "year": record.get("pubYear"),
                "abstract": record.get("abstractText"),
            }

        except requests.exceptions.Timeout:
            return {"success": False, "error": "Request timeout", "doi": doi}
        except requests.exceptions.ConnectionError:
            return {"success": False, "error": "Connection error", "doi": doi}
        except Exception as e:
            return {"success": False, "error": str(e), "doi": doi}

    def _download_pdf(self, pmcid: str, doi: str) -> dict:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        # å°è¯•å‡ ä¸ªå¸¸è§çš„PDF URL
        pdf_urls = [
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/",
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/{pmcid}.pdf",
            f"https://europepmc.org/articles/{pmcid}?pdf=render",
        ]

        for i, pdf_url in enumerate(pdf_urls):
            try:
                self.logger.debug(f"  ğŸ“¥ å°è¯•PDFæº {i + 1}: {pdf_url}")
                response = self.session.get(pdf_url, timeout=30, stream=True)
                response.raise_for_status()

                content_type = response.headers.get("content-type", "").lower()
                if "application/pdf" not in content_type:
                    continue

                # ä¿å­˜æ–‡ä»¶
                safe_doi = "".join(c for c in doi if c.isalnum() or c in "-._")
                filename = f"{pmcid}_{safe_doi}.pdf"
                file_path = self.output_dir / filename

                with open(file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                self.logger.info(f"  ğŸ’¾ PDFä¿å­˜æˆåŠŸ: {file_path}")
                return {"success": True, "path": str(file_path)}

            except Exception as e:
                self.logger.debug(f"  âš ï¸ PDFæº {i + 1} å¤±è´¥: {str(e)}")
                continue

        return {"success": False, "error": "All PDF sources failed"}

    def _get_cache(self, doi: str) -> dict | None:
        """ç®€å•ç¼“å­˜æ£€æŸ¥"""
        cache_file = (
            self.cache_dir / f"cache_{hashlib.md5(doi.encode()).hexdigest()}.json"
        )

        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    data = json.load(f)

                # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
                if data.get("pdf_path") and not Path(data["pdf_path"]).exists():
                    self.logger.debug("ç¼“å­˜çš„PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ¸…é™¤ç¼“å­˜")
                    cache_file.unlink()
                    return None

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                if time.time() - data.get("timestamp", 0) > 86400:
                    self.logger.debug("ç¼“å­˜å·²è¿‡æœŸ")
                    cache_file.unlink()
                    return None

                return data  # type: ignore

            except Exception as e:
                self.logger.debug(f"ç¼“å­˜è¯»å–å¤±è´¥: {str(e)}")
                cache_file.unlink()
                return None

        return None

    def _save_cache(self, doi: str, result: dict) -> None:
        """ä¿å­˜ç¼“å­˜"""
        try:
            cache_file = (
                self.cache_dir / f"cache_{hashlib.md5(doi.encode()).hexdigest()}.json"
            )
            result["timestamp"] = time.time()

            with open(cache_file, "w") as f:
                json.dump(result, f, indent=2)

        except Exception as e:
            self.logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")

    def fetch_batch(
        self, dois: list[str] | list[dict], delay: float = 1.0
    ) -> list[dict]:
        """
        æ‰¹é‡è·å–æ–‡çŒ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        Args:
            dois: DOIåˆ—è¡¨æˆ–è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            delay: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            ç»“æœåˆ—è¡¨
        """
        # æ£€æŸ¥è¾“å…¥æ ¼å¼
        papers: list[dict] = []
        if dois and isinstance(dois[0], dict):
            # è¾“å…¥æ˜¯è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            papers = dois  # type: ignore
            dois = [p["doi"] for p in papers if p.get("doi")]  # type: ignore
        else:
            # è¾“å…¥æ˜¯DOIåˆ—è¡¨ï¼Œæ²¡æœ‰PMCIDä¿¡æ¯
            papers = [{"doi": d} for d in dois]

        self.logger.info(f"ğŸš€ æ‰¹é‡è·å– {len(dois)} ç¯‡æ–‡çŒ®")
        results = []

        for i, paper in enumerate(papers, 1):
            doi = paper["doi"] if isinstance(paper, dict) else paper
            pmcid = paper.get("pmcid") if isinstance(paper, dict) else None

            self.logger.info(f"\nğŸ“„ è¿›åº¦: {i}/{len(papers)}")

            try:
                result = self.fetch_by_doi(str(doi), pmcid=pmcid)
                results.append(result)
            except Exception as e:
                self.logger.error(f"è·å–æ–‡çŒ®å¤±è´¥ ({doi}): {e}")
                results.append({"doi": doi, "success": False, "error": str(e)})

            # ç®€å•å»¶è¿Ÿï¼Œé¿å…è¢«é™åˆ¶
            if i < len(papers):
                time.sleep(delay)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r.get("success"))
        self.logger.info(f"\nğŸ“Š æ‰¹é‡è·å–å®Œæˆ: {success_count}/{len(dois)} æˆåŠŸ")

        return results
