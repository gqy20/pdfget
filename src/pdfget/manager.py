#!/usr/bin/env python3
"""
ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨
æ ¹æ®å‚æ•°è‡ªåŠ¨é€‰æ‹©å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹ä¸‹è½½ç­–ç•¥
"""

import random
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

from .config import DOWNLOAD_BASE_DELAY, DOWNLOAD_RANDOM_DELAY
from .fetcher import PaperFetcher
from .logger import get_logger


class UnifiedDownloadManager:
    """ç»Ÿä¸€ä¸‹è½½ç®¡ç†å™¨ï¼Œæ”¯æŒå•çº¿ç¨‹å’Œå¤šçº¿ç¨‹ä¸‹è½½"""

    def __init__(
        self,
        fetcher: PaperFetcher,
        max_workers: int = 1,
        base_delay: float = DOWNLOAD_BASE_DELAY,
        random_delay_range: float = DOWNLOAD_RANDOM_DELAY,
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½ç®¡ç†å™¨

        Args:
            fetcher: PaperFetcherå®ä¾‹
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
            base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
            random_delay_range: éšæœºå»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
        """
        self.logger = get_logger(__name__)
        self.fetcher = fetcher
        self.max_workers = max_workers
        self.base_delay = base_delay
        self.random_delay_range = random_delay_range

        # çº¿ç¨‹å®‰å…¨çš„è¿›åº¦è·Ÿè¸ªï¼ˆä»…å¤šçº¿ç¨‹ä½¿ç”¨ï¼‰
        self._lock = threading.Lock()
        self._completed = 0
        self._successful = 0
        self._failed = 0
        self._pdf_count = 0
        self._total = 0

    def _normalize_input(
        self, items: list[str] | list[dict]
    ) -> tuple[list[dict], list[str]]:
        """
        æ ‡å‡†åŒ–è¾“å…¥æ ¼å¼

        Args:
            items: DOIåˆ—è¡¨æˆ–è®ºæ–‡ä¿¡æ¯åˆ—è¡¨

        Returns:
            (è®ºæ–‡ä¿¡æ¯åˆ—è¡¨, DOIåˆ—è¡¨)
        """
        papers: list[dict] = []
        if items and isinstance(items[0], dict):
            # è¾“å…¥æ˜¯è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            papers = items  # type: ignore
            dois = [p["doi"] for p in papers if p.get("doi")]  # type: ignore
        else:
            # è¾“å…¥æ˜¯DOIåˆ—è¡¨
            papers = [{"doi": d} for d in items]
            dois = items  # type: ignore

        return papers, dois

    def _get_delay(self) -> float:
        """è·å–éšæœºå»¶è¿Ÿæ—¶é—´ï¼Œé¿å…åŒæ­¥è¯·æ±‚"""
        if self.random_delay_range > 0:
            random_delay = random.uniform(0, self.random_delay_range)
            return self.base_delay + random_delay
        return self.base_delay

    def _update_progress(
        self, success: bool = False, pdf_downloaded: bool = False
    ) -> None:
        """çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ›´æ–°"""
        with self._lock:
            self._completed += 1
            if success:
                self._successful += 1
                if pdf_downloaded:
                    self._pdf_count += 1
            else:
                self._failed += 1

            # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
            progress = (self._completed / self._total) * 100
            self.logger.info(
                f"  è¿›åº¦: {self._completed}/{self._total} ({progress:.1f}%) "
                f"æˆåŠŸ: {self._successful} PDF: {self._pdf_count} å¤±è´¥: {self._failed}"
            )

    def _create_thread_fetcher(self) -> PaperFetcher:
        """ä¸ºçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„fetcherå®ä¾‹"""
        # å¤åˆ¶åŸºç¡€é…ç½®ï¼Œä½†åˆ›å»ºæ–°çš„session
        fetcher = PaperFetcher(
            cache_dir=str(self.fetcher.cache_dir),
            output_dir=str(self.fetcher.output_dir),
        )
        return fetcher

    def _download_concurrent(self, papers: list[dict], timeout: int = 30) -> list[dict]:
        """
        å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½

        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        self.logger.info(
            f"ğŸš€ å¯åŠ¨å¹¶å‘ä¸‹è½½ï¼š{len(papers)} ç¯‡æ–‡çŒ®ï¼Œ{self.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹"
        )

        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
        self._total = len(papers)
        self._completed = 0
        self._successful = 0
        self._failed = 0
        self._pdf_count = 0

        results = []

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_identifier = {}

            for paper in papers:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„fetcher
                thread_fetcher = self._create_thread_fetcher()
                doi = paper.get("doi")
                pmcid = paper.get("pmcid")
                identifier = doi or pmcid  # ä½¿ç”¨ DOI æˆ– PMCID ä½œä¸ºæ ‡è¯†ç¬¦

                future = executor.submit(
                    self._download_single_task,
                    doi,
                    pmcid,
                    thread_fetcher,
                    timeout,
                )
                future_to_identifier[future] = (doi, pmcid)

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_identifier):
                doi, pmcid = future_to_identifier[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    identifier = doi or pmcid or "unknown"
                    self.logger.error(f"å¹¶å‘ä¸‹è½½å¼‚å¸¸ ({identifier}): {str(e)}")
                    results.append(
                        {"doi": doi, "pmcid": pmcid, "success": False, "error": str(e)}
                    )

        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—ç»“æœ
        # åˆ›å»ºä¸€ä¸ªæ ‡è¯†ç¬¦åˆ°ç»“æœçš„æ˜ å°„ï¼ˆä¼˜å…ˆä½¿ç”¨DOIï¼Œå…¶æ¬¡æ˜¯PMCIDï¼‰
        identifier_to_result = {}
        for r in results:
            if r.get("doi"):
                identifier_to_result[r["doi"]] = r
            elif r.get("pmcid"):
                identifier_to_result[r["pmcid"]] = r

        # æŒ‰åŸå§‹è®ºæ–‡é¡ºåºæ’åˆ—ç»“æœ
        ordered_results = []
        for paper in papers:
            doi = paper.get("doi")
            pmcid = paper.get("pmcid")
            identifier = doi or pmcid

            if identifier:
                lookup_result = identifier_to_result.get(identifier)
                if lookup_result:
                    ordered_results.append(lookup_result)
                else:
                    # åˆ›å»ºé»˜è®¤å¤±è´¥ç»“æœ
                    ordered_results.append(
                        {
                            "doi": doi or "",
                            "pmcid": pmcid or "",
                            "success": False,
                            "error": "Not found",
                        }
                    )

        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info("\nğŸ“Š å¹¶å‘ä¸‹è½½å®Œæˆ:")
        self.logger.info(f"   æ€»è®¡: {len(ordered_results)}")
        self.logger.info(f"   æˆåŠŸ: {self._successful}")
        self.logger.info(f"   PDF: {self._pdf_count}")
        self.logger.info(f"   å¤±è´¥: {self._failed}")
        if len(ordered_results) > 0:
            self.logger.info(
                f"   æˆåŠŸç‡: {(self._successful / len(ordered_results)) * 100:.1f}%"
            )

        return ordered_results

    def _download_single_task(
        self, doi: str, pmcid: str | None, fetcher: PaperFetcher, timeout: int = 30
    ) -> dict[str, Any]:
        """å•ä¸ªæ–‡çŒ®çš„ä¸‹è½½ä»»åŠ¡ï¼ˆçº¿ç¨‹æ± ä¸­çš„ä»»åŠ¡ï¼‰"""
        try:
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(self._get_delay())

            # ç›´æ¥ä½¿ç”¨PDFDownloaderä¸‹è½½
            if pmcid:
                result = fetcher.pdf_downloader.download_pdf(pmcid, doi)
            else:
                # å¦‚æœæ²¡æœ‰PMCIDï¼Œå°è¯•æœç´¢
                papers = fetcher.search_papers(doi, limit=1)
                if papers and papers[0].get("pmcid"):
                    pmcid = papers[0]["pmcid"]
                    result = fetcher.pdf_downloader.download_pdf(pmcid, doi)
                else:
                    result = {"success": False, "error": "No PMCID found"}

            # æ·»åŠ å¿…è¦çš„ä¿¡æ¯
            result["doi"] = doi
            result["pmcid"] = pmcid or ""

            # æ›´æ–°è¿›åº¦
            success = result.get("success", False)
            pdf_downloaded = bool(result.get("path"))
            self._update_progress(success, pdf_downloaded)

            return result

        except Exception as e:
            self.logger.debug(f"ä¸‹è½½å¤±è´¥ ({doi}): {str(e)}")
            self._update_progress(False)
            return {"doi": doi, "success": False, "error": str(e)}

    def download_batch(
        self,
        items: list[str] | list[dict],
        timeout: int = 30,
    ) -> list[dict[str, Any]]:
        """
        æ‰¹é‡ä¸‹è½½æ–‡çŒ®ï¼ˆä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼‰

        Args:
            items: DOIåˆ—è¡¨æˆ–è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            timeout: å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨
        """
        if not items:
            return []

        # æ ‡å‡†åŒ–è¾“å…¥
        papers, dois = self._normalize_input(items)

        # æ£€æŸ¥æ˜¯å¦æœ‰ PMCIDï¼ˆå³ä½¿æ²¡æœ‰ DOI ä¹Ÿå¯ä»¥ä¸‹è½½ï¼‰
        pmcid_count = sum(1 for p in papers if p.get("pmcid"))

        if not dois and not pmcid_count:
            self.logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„DOIæˆ–PMCIDå¯ä»¥ä¸‹è½½")
            return []

        # æ€»æ˜¯ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼ˆmax_workers=1 æ—¶ç›¸å½“äºå•çº¿ç¨‹ï¼‰
        return self._download_concurrent(papers, timeout)
